{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Anti-Spoofing.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYRWxE5c-skt"
      },
      "source": [
        "!pip install kaggle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDN08Fq1_Bt2"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUXzt2oP_Jix"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJHZIjeQ_Qm7"
      },
      "source": [
        "!kaggle datasets download -d boksman/spoof-raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_K29d6k_Vkb"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = 'spoof-raw.zip'\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBkyXin4BBG9"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "import shutil # high-level operations on files\n",
        "from tqdm import tqdm # Progress bar and status logging\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "import cv2 # computer vision algorithms\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "import tensorflow as tf\n",
        "from keras import utils\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import to_categorical \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hWC2sXzBGd7"
      },
      "source": [
        "# Configuration\n",
        "import io, os, sys, types\n",
        "DATASET_DIR = '/Users/zunay/Desktop/kaggle/data'\n",
        "TRAIN_DIR = '/Users/zunay/Desktop/kaggle/data/IDRND_FASDB_train'\n",
        "TEST_DIR = '/Users/zunay/Desktop/kaggle/data/test'\n",
        "\n",
        "RATE = 0.2 # splitting proportion for training and test datasets\n",
        "\n",
        "# Parameters for Grid Search\n",
        "\n",
        "N_EPOCHS = [20] #[20, 40, 100, 200]\n",
        "OPTIMIZERS = ['adam'] #['adam', 'rmsprop', 'SGD']\n",
        "DROPOUT_RATES =  [0.1, 0.2, 0.4]\n",
        "LOSS_FUNCTIONS = ['binary_crossentropy']  #['sparse_categorical_crossentropy', 'kullback_leibler_divergence']   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm4Fnq0vB8q9"
      },
      "source": [
        "train_samples = sum([len(files) for r, d, files in os.walk(TRAIN_DIR)])\n",
        "test_samples = sum([len(files) for r, d, files in os.walk(TEST_DIR)])\n",
        "print('Number of training images: {} \\nNumber of test images: {}'.format(train_samples, test_samples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAhEfkW_CRSL"
      },
      "source": [
        "def get_images(path, img_shape=(64, 64)):\n",
        " \n",
        "    '''\n",
        "    Returns a np array of images and labels from path\n",
        "    Images must be stored in path/class1, path/class2\n",
        "    '''\n",
        "    main_path = path\n",
        "    y = []\n",
        "    list1 = [name for name in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, name))]\n",
        "    print(list1)\n",
        "    image_collection = []\n",
        "    for idx,folder in enumerate(list1):\n",
        " \n",
        "        label = idx\n",
        "        \n",
        "        sub_list = sorted(os.listdir(os.path.join(main_path,folder)))\n",
        " \n",
        "        for i in tqdm(range(1, len(sub_list))):\n",
        "            image_path = os.path.join(main_path, folder, sub_list[i])\n",
        "            read_image = cv2.imread(image_path)\n",
        "            image_resized = cv2.resize(read_image, img_shape, interpolation=cv2.INTER_AREA)\n",
        " \n",
        "            image = np.float32(image_resized)\n",
        "            image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F) #Change alpha, beta according to the preprocessing you desire\n",
        "            \n",
        "            image_collection.append(image)\n",
        "            \n",
        "            y.append(label)\n",
        " \n",
        "    y = np.array(y)\n",
        "    y = utils.to_categorical(y,num_classes=len(list1))\n",
        " \n",
        "    return image_collection, y[:,0] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cji0UjH8CUur"
      },
      "source": [
        "# Preparing test and trainng datasets\n",
        "X_train,y_train = get_images(TRAIN_DIR,img_shape=(64,64))\n",
        "X_test,y_test = get_images(TEST_DIR,img_shape=(64,64))\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "print(X_train.shape)\n",
        "# print(X_train[0])\n",
        "#from PIL import Image\n",
        "#im = Image.fromarray(X_train[0].astype('uint8'))\n",
        "#im.save(\"img50.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVzX0rU4CmFk"
      },
      "source": [
        "print('Training set', X_train.shape)\n",
        "print('Test set', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVnYTRHJIwBF"
      },
      "source": [
        "#Shuffle training examples\n",
        "X_train, y_train = shuffle(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai9q3Tj5V4vm"
      },
      "source": [
        "def build_classifier(optimizer, dropout, loss):\n",
        "    classifier = Sequential() # Initialising the CNN    \n",
        "    classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu')) \n",
        "    classifier.add(MaxPooling2D(pool_size = (2, 2))) \n",
        "    classifier.add(Dropout(dropout))\n",
        "    classifier.add(Conv2D(32, (3, 3), activation = 'relu'))  \n",
        "    classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    classifier.add(Dropout(dropout))\n",
        "    classifier.add(Conv2D(32, (3, 3), activation = 'relu'))  \n",
        "    classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    classifier.add(Dropout(dropout))\n",
        "    classifier.add(Flatten())\n",
        "    classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, activation = 'sigmoid')) #'tanh'))\n",
        "    \n",
        "    classifier.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
        "    \n",
        "    return classifier\n",
        "\n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "\n",
        "grid_parameters = {'epochs': N_EPOCHS,\n",
        "                  'optimizer': OPTIMIZERS,\n",
        "                  'dropout': DROPOUT_RATES,                  \n",
        "                  'loss':LOSS_FUNCTIONS                        \n",
        "                  }\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = grid_parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10)\n",
        "\n",
        "\n",
        "grid_search = grid_search.fit(X_train, y_train, verbose=0)\n",
        "\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeHIDPa1gtll"
      },
      "source": [
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "print(best_parameters)\n",
        "print(best_accuracy)\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWyP7rp9L7B6"
      },
      "source": [
        "predicted = grid_search.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-YpOrb0YXBp"
      },
      "source": [
        "print('Confusion matrix for training set:')\n",
        "print(confusion_matrix(y_train,grid_search.predict(X_train)))\n",
        "print('\\n')\n",
        "print(classification_report(y_train,grid_search.predict(X_train)))\n",
        "\n",
        "print('Confusion matrix  for test set:')\n",
        "print(confusion_matrix(y_test,predicted))\n",
        "print('\\n')\n",
        "print(classification_report(y_test,predicted))\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}